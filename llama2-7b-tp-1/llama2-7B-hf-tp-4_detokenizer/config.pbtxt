name: "llama2-7B-hf-tp-4_detokenizer"
backend: "python"
max_batch_size: 4096
input [
    {
      name: "tokens_batch"
      data_type: TYPE_INT32
      dims: [ -1, -1 ]
    },
    {
      name: "input_sequence_lengths"
      data_type: TYPE_INT32
      dims: [1] 
      reshape: { shape: [ ] }
    }
  ]
  output [
    {
      name: "output"
      data_type: TYPE_STRING
      dims: [ -1, -1 ]
    }
  ]
instance_group [
    {
        count: 1
        # Tokenizer only needs CPU
        kind: KIND_CPU
    }
]
